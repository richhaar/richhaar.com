"use strict";(self.webpackChunkdocusaurus=self.webpackChunkdocusaurus||[]).push([[251],{1180:function(e,n,t){t.r(n),t.d(n,{frontMatter:function(){return l},contentTitle:function(){return o},metadata:function(){return c},assets:function(){return d},toc:function(){return f},default:function(){return p}});var a=t(7462),i=t(3366),r=(t(7294),t(3905)),s=["components"],l={slug:"multivariable-linear-regression",title:"Multivariable linear regression in Python",authors:"rich",tags:["Python","MachineLearning"]},o=void 0,c={permalink:"/multivariable-linear-regression",source:"@site/blog/2021-09-26-multivariable-linear-regression.md",title:"Multivariable linear regression in Python",description:"Multivariable linear regression is all about taking in a set of data points (x0, x1, &#x2026;,xn, y) and to be able to predict y values for some other data points (x0',x1',&#x2026;,xn'). I'm giving an example here on how to do so in Python as well as computing the coefficient of determination (R2) to see how well the predictor variables model y.",date:"2021-09-26T00:00:00.000Z",formattedDate:"September 26, 2021",tags:[{label:"Python",permalink:"/tags/python"},{label:"MachineLearning",permalink:"/tags/machine-learning"}],readingTime:11.98,truncated:!0,authors:[{name:"Richard Haar",title:"Computer Scientist",url:"https://github.com/richhaar",imageURL:"/img/profile3.png",key:"rich"}],nextItem:{title:"Making it to the front page on StackOverflow",permalink:"/stackoverflow"}},d={authorsImageUrls:[void 0]},f=[{value:"Input",id:"input",children:[{value:"Dataset",id:"dataset",children:[]},{value:"Goal",id:"goal",children:[]}]},{value:"Output",id:"output",children:[{value:"Results",id:"results",children:[]},{value:"Graphs of the correlation",id:"graphs-of-the-correlation",children:[]}]},{value:"Regression on spring fawn count",id:"regression-on-spring-fawn-count",children:[]},{value:"Output",id:"output-1",children:[]},{value:"Source Code",id:"source-code",children:[]}],u={toc:f};function p(e){var n=e.components,t=(0,i.Z)(e,s);return(0,r.kt)("wrapper",(0,a.Z)({},u,t,{components:n,mdxType:"MDXLayout"}),(0,r.kt)("p",null,"Multivariable linear regression is all about taking in a set of data points (x",(0,r.kt)("sub",null,"0"),", x",(0,r.kt)("sub",null,"1"),", ","\u2026",",x",(0,r.kt)("sub",null,"n"),", y) and to be able to predict y values for some other data points (x",(0,r.kt)("sub",null,"0"),"',x",(0,r.kt)("sub",null,"1"),"',","\u2026",",x",(0,r.kt)("sub",null,"n"),"'). I'm giving an example here on how to do so in Python as well as computing the coefficient of determination (R",(0,r.kt)("sup",null,"2"),") to see how well the predictor variables model y."),(0,r.kt)("p",null,"All the following code is available with an MIT licence ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/richhaar/multivariable-linear-regression/"},"here"),"."),(0,r.kt)("h2",{id:"input"},"Input"),(0,r.kt)("h3",{id:"dataset"},"Dataset"),(0,r.kt)("p",null," The example data has been adapted from the Thunder Basin Antelope study found ",(0,r.kt)("a",{href:"https://college.cengage.com/mathematics/brase/understandable_statistics/7e/students/datasets/mlr/frames/mlr01.html"},"online here"),".\nThe data contains four variables (each corresponding to a year), the first being the spring fawn count(/100),\nthe second being annual precipitation (inches),\nthe third being the winter severity index (1=mild, 5=severe),\nand the fourth variable (which is the one which will act as the dependent variable y) is the size of the adult      antelope population(/100)."),(0,r.kt)("table",null,(0,r.kt)("thead",{parentName:"table"},(0,r.kt)("tr",{parentName:"thead"},(0,r.kt)("th",{parentName:"tr",align:null},"Spring fawn count/100"),(0,r.kt)("th",{parentName:"tr",align:null},"Annual precipitation(inches)"),(0,r.kt)("th",{parentName:"tr",align:null},"Winter severity index(1=mild;5=severe)"),(0,r.kt)("th",{parentName:"tr",align:null},"Size of adult antelope population/100"))),(0,r.kt)("tbody",{parentName:"table"},(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"2.900000095"),(0,r.kt)("td",{parentName:"tr",align:null},"13.19999981"),(0,r.kt)("td",{parentName:"tr",align:null},"2"),(0,r.kt)("td",{parentName:"tr",align:null},"9.199999809")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"2.400000095"),(0,r.kt)("td",{parentName:"tr",align:null},"11.5"),(0,r.kt)("td",{parentName:"tr",align:null},"3"),(0,r.kt)("td",{parentName:"tr",align:null},"8.699999809")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"2"),(0,r.kt)("td",{parentName:"tr",align:null},"10.80000019"),(0,r.kt)("td",{parentName:"tr",align:null},"4"),(0,r.kt)("td",{parentName:"tr",align:null},"7.199999809")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"2.299999952"),(0,r.kt)("td",{parentName:"tr",align:null},"12.30000019"),(0,r.kt)("td",{parentName:"tr",align:null},"2"),(0,r.kt)("td",{parentName:"tr",align:null},"8.5")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"3.200000048"),(0,r.kt)("td",{parentName:"tr",align:null},"12.60000038"),(0,r.kt)("td",{parentName:"tr",align:null},"3"),(0,r.kt)("td",{parentName:"tr",align:null},"9.6")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"1.899999976"),(0,r.kt)("td",{parentName:"tr",align:null},"10.60000038"),(0,r.kt)("td",{parentName:"tr",align:null},"5"),(0,r.kt)("td",{parentName:"tr",align:null},"6.800000191")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"3.400000095"),(0,r.kt)("td",{parentName:"tr",align:null},"14.10000038"),(0,r.kt)("td",{parentName:"tr",align:null},"1"),(0,r.kt)("td",{parentName:"tr",align:null},"9.699999809")),(0,r.kt)("tr",{parentName:"tbody"},(0,r.kt)("td",{parentName:"tr",align:null},"2.099999905"),(0,r.kt)("td",{parentName:"tr",align:null},"11.19999981"),(0,r.kt)("td",{parentName:"tr",align:null},"3"),(0,r.kt)("td",{parentName:"tr",align:null},"7.900000095")))),(0,r.kt)("h3",{id:"goal"},"Goal"),(0,r.kt)("p",null,"The goal is to determine if the first three variables are good predictors of the adult antelope population size. And if they are, to make predictions of adult antelope population based on new data that only contains the first three variables (spring fawn count, annual precipitation and winter severity)."),(0,r.kt)("h2",{id:"output"},"Output"),(0,r.kt)("p",null,"The below code handles the data acquisition from the csv files, and sends it to the MultivariableRegression class.\nThere is the option to add a file to process a list of prediction data and whether to plot data."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="main.py"',title:'"main.py"'},'import numpy as np\nimport csv\nimport mvr\n\n\nclass Main:\n\n    filename  = "antelopestudy.csv"\n    predict   = "antelope_predict.csv"\n    delimiter = \',\'\n\n    # Load CSV headers only\n    with open(filename) as file:\n        reader = csv.reader(file)\n        headers = next(reader)\n\n    # Load remaining CSV data\n    data = np.loadtxt(filename, delimiter=delimiter, skiprows=1)\n    prediction_data = np.loadtxt(predict, delimiter=delimiter, skiprows=1)\n\n    # Perform the regression\n    r = mvr.MultiVariableRegression(data, headers, estimates=prediction_data)\n    r.plotData()\n')),(0,r.kt)("h3",{id:"results"},"Results"),(0,r.kt)("p",null," An analysis of variance test is run on the data. The analysis is run with a null hypothesis to check whether the population regression coefficients (A",(0,r.kt)("sub",null,"0"),", A",(0,r.kt)("sub",null,"1"),", A",(0,r.kt)("sub",null,"2"),") are 0.\nFrom the output, A",(0,r.kt)("sub",null,"0")," (spring fawn count) is found to be a good predictor of the dependent variable.\nHowever the null hypothesis is accepted for both A",(0,r.kt)("sub",null,"1")," (annual precipitation) and A",(0,r.kt)("sub",null,"2")," (winter      severity index),\nmeaning we cannot say with 95% confidence that annual precipitation and the winter severity affects the adult        antelope population."),(0,r.kt)("div",{className:"admonition admonition-info alert alert--info"},(0,r.kt)("div",{parentName:"div",className:"admonition-heading"},(0,r.kt)("h5",{parentName:"div"},(0,r.kt)("span",{parentName:"h5",className:"admonition-icon"},(0,r.kt)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"},(0,r.kt)("path",{parentName:"svg",fillRule:"evenodd",d:"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"}))),"Note")),(0,r.kt)("div",{parentName:"div",className:"admonition-content"},(0,r.kt)("p",{parentName:"div"},"With significance of 95%, there is a 5% chance we reject the null hypothesis when it is true."))),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"Running Analysis of Variance with given significance:0.95\nNull hypothesis: A_0...A_n are all 0\nNull hypotehesis rejected 6.59 < 27.1\nAt least one A_x is not 0 with certainty 0.95\nNull hypotehesis rejected 6.59 < 11.6\nA_0 is not 0 with certainty 0.95\nNull hpothesis accepted6.59 > 2.29\nA_1 is 0 with certainty 0.95\nNull hpothesis accepted6.59 > 5.4\nA_2 is 0 with certainty 0.95\nFor predictor variables: (0,): regression = [1.77142829 3.97714348]R^2 = 0.8615637849525227\nFor predictor variables: (1,): regression = [ 0.78979077 -1.05710652]R^2 = 0.7837637022913508\nFor predictor variables: (2,): regression = [-0.72183902 10.52528711]R^2 = 0.6494867453327626\nFor predictor variables: (0, 1): regression = [1.35202677 0.21051042 2.50211306]R^2 = 0.8457378818812388\nFor predictor variables: (0, 2): regression = [ 1.33311064 -0.27133953  5.86399668]R^2 = 0.89672011835548\nFor predictor variables: (1, 2): regression = [ 0.69234295 -0.10668834  0.42265053]R^2 = 0.7445482721426719\nFor predictor variables: (0, 1, 2): regression = [ 2.19667031 -0.70400663 -0.60502981 13.11734799]R^2 = 0.917920937434756\nx_n [ 2.5 14.   3. ] y_hat 6.94 ci 2.67 pi 2.8 with confidence 0.95\n")),(0,r.kt)("h3",{id:"graphs-of-the-correlation"},"Graphs of the correlation"),(0,r.kt)("p",null,(0,r.kt)("img",{parentName:"p",src:"https://raw.githubusercontent.com/richhaar/multivariable-linear-regression/531034ce5314e933aa36e8c9f81b57b389f04de3/Figure_0.png",alt:"Figure 0"})),(0,r.kt)("p",null,(0,r.kt)("img",{parentName:"p",src:"https://raw.githubusercontent.com/richhaar/multivariable-linear-regression/531034ce5314e933aa36e8c9f81b57b389f04de3/Figure_1.png",alt:"Figure 1"})),(0,r.kt)("p",null,(0,r.kt)("img",{parentName:"p",src:"https://raw.githubusercontent.com/richhaar/multivariable-linear-regression/531034ce5314e933aa36e8c9f81b57b389f04de3/Figure_2.png",alt:"Figure 2"})),(0,r.kt)("p",null," Additionally you can see the coefficient of determination (R",(0,r.kt)("sup",null,"2"),") is highest for the 0th predictor (spring fawn count).\nBased on this information, more data needs to be gathered to see if annual precipitation and winter severity can be a good indicator of population size."),(0,r.kt)("h2",{id:"regression-on-spring-fawn-count"},"Regression on spring fawn count"),(0,r.kt)("p",null,"The regression can be run again, using only the spring fawn count as the only independent variable.\nIt is possible to quickly make a new dataset by making use of cut, to select columns 1 and 4.\nThen the main file gets updated to handle the new file, and the prediction file."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sh"},'cat antelopestudy.csv | cut -d"," -f1,4 > antelopestudy_2.csv \n')),(0,r.kt)("h2",{id:"output-1"},"Output"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"Running Analysis of Variance with given significance:0.95\nNull hypothesis: A_0...A_n are all 0\nNull hypotehesis rejected 5.99 < 44.6\nAtleast one A_x is not 0 with certainty 0.95\nFor predictor variables: (0,): regression = [1.77142829 3.97714348]R^2 = 0.8615637849525227\nx_n [2.5] y_hat 8.41 ci 0.347 pi 1.04 with confidence 0.95\n")),(0,r.kt)("p",null,(0,r.kt)("img",{parentName:"p",src:"https://raw.githubusercontent.com/richhaar/multivariable-linear-regression/531034ce5314e933aa36e8c9f81b57b389f04de3/Figure_3.png",alt:"Figure 3"})),(0,r.kt)("p",null,"The output regression equation is 1.77a + 3.98. So for some given fawn count, an estimate can be given of the adult  antelope count."),(0,r.kt)("p",null,"So given the fact that the spring fawn count is 250, an estimate is given of 841 adult antelopes. If there were many fawn counts of 250, the expected mean adult antelopes is expected to fall within the predicted    (841) plus or minus the confidence interval (0.347) which represents a hundered. So with 0.95 confidence and spring fawn count of 250, the mean adult antelopes is expected to fall within 841+-35."),(0,r.kt)("p",null,"With observed readings (with 0.95 confidence) falling in 841+-104."),(0,r.kt)("h2",{id:"source-code"},"Source Code"),(0,r.kt)("p",null,"Again all the source code is available on ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/richhaar/multivariable-linear-regression/"},"GitHub")," under a MIT licence."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="mvr.py"',title:'"mvr.py"'},"import numpy as np\nimport matplotlib.pyplot as plt\nimport itertools\n\nimport mvr_calc\n\nclass MultiVariableRegression:\n    \"\"\"\n    Class to manage the multivariable regression, holding a data class,\n    which has it's own calculation class to handle regression calculations.\n    \"\"\"\n\n    class Data:\n        \"\"\"Holds the raw numerical data for the regression calculations\"\"\"\n        def sanityCheck(self):\n            \"\"\"\n            Test statistics rely on the degrees of freedom being\n            greater than 0, Having more data than the number of\n            variable predictors (x.. x_n) by atleast 2 will satisfy\n            the sanity check.\n            \"\"\"\n            if self.df > 0:\n                self.sane = True\n\n        def populateMetaData(self):\n            self.ndim  = self.c.getMatrixWidth(self.x)\n            self.ndata = self.y.size\n            self.df    = self.ndata - self.ndim - 1\n\n            self.x_matrix = self.c.addOnesToData(self.x, self.ndata, self.ndim)\n\n        def populateVariance(self):\n            self.y_bar = self.c.calcAverage(self.y)\n            self.x_bar = self.c.calcAverage(self.x)\n\n            self.y_variance = self.c.calcVariance(self.y,self.y_bar)\n            self.x_variance = self.c.calcVariance(self.x,self.x_bar).reshape(\n                self.ndata, self.ndim)\n\n            self.y_variance_sq = self.c.calcSumProduct(\n                self.y_variance,self.y_variance)\n\n            self.x_variance_sq = np.zeros(self.ndim)\n            self.x_y_variance  = np.zeros(self.ndim)\n\n            for n in range(0, self.ndim):\n                x_var = self.x_variance[:,n]\n                self.x_variance_sq[n] = self.c.calcSumProduct(\n                    x_var, x_var)\n                self.x_y_variance[n]  = self.c.calcSumProduct(\n                    x_var, self.y_variance)\n\n        def populateCorrelation(self):\n            self.correlation = self.c.calcCorrelation(\n                self.ndim,\n                self.x_y_variance,\n                self.x_variance_sq,\n                self.y_variance_sq)\n\n        def populateRegression(self):\n            self.s_matrix = self.c.findSMatrix(self.x_matrix)\n            self.regression = self.c.calcRegression(\n                self.s_matrix, self.x_matrix, self.y)\n\n        def populateEstimationData(self):\n            self.y_hat   = np.dot(self.x_matrix, self.regression)\n            self.y_error = self.y - self.y_hat\n\n            self.sum_errors_sq = self.c.calcSumProduct(\n                self.y_error, self.y_error)\n\n            self.adjusted_R_sq = self.c.findAdjustedRSquared(\n                self.sum_errors_sq, self.y_variance_sq, self.ndata, self.df)\n\n        def __init__(self,x,y,c):\n            self.x = x\n            self.y = y\n            self.c = c\n            self.populateMetaData()\n            self.sanityCheck()\n            if self.sane:\n                self.populateVariance()\n                self.populateCorrelation()\n                self.populateRegression()\n                self.populateEstimationData()\n\n    def ANOVA(self,core_data,significance):\n        \"\"\"\n        Run the analysis of variance test on the data. The analysis\n        is run with a null hypothesis to check whether the population\n        regression coefficients (A_0 ... A_n) are 0. Such that the data\n        follows a normal distribution with mean B and standard deviation\n        sigma. By rejecting the hypothesis, we can say with the given\n        significance the x dimensions do affect the y data. And\n        uses the F distribution to determine the critical value.\n\n        Run once for All A_0,... A_n being 0, which can be rejected\n        if any A is not 0. Then run for each A_x.\n        \"\"\"\n        print(f'Running Analysis of Variance with given significance:'\n              + f'{significance}\\nNull hypothesis: A_0...A_n are all 0')\n\n        test_statistic = self.c.calcTestStatisticAllX(\n            core_data.y_variance_sq,\n            core_data.sum_errors_sq,\n            core_data.ndim,\n            core_data.df)\n\n        critical_value = core_data.c.findCriticalFValue(\n            core_data.ndim,\n            core_data.df,\n            significance)\n\n        if critical_value < test_statistic:\n            print(f'Null hypotehesis rejected '\n                  + f'{critical_value:.3} < {test_statistic:.3}\\n'\n                  + f'Atleast one A_x is not 0 with certainty {significance}')\n        else:\n            print(f'Null hpothesis accepted'\n                  + f'{critical_value:.3} > {test_statistic:.3}\\n'\n                  + f'All A_x are 0 with certainty {significance}')\n\n        if core_data.ndim > 1:\n            for n in range(0,core_data.ndim):\n                test_statistic = self.c.calcTestStatisticSingleX(\n                    core_data.regression,\n                    core_data.s_matrix,\n                    core_data.sum_errors_sq,\n                    n,\n                    core_data.df)\n\n                critical_value = core_data.c.findCriticalFValue(\n                    core_data.ndim,\n                    core_data.df,\n                    significance)\n\n                if critical_value < test_statistic:\n                    print(f'Null hypotehesis rejected '\n                          + f'{critical_value:.3} < {test_statistic:.3}\\n'\n                          + f'A_{n} is not 0 with certainty {significance}')\n                else:\n                    print(f'Null hpothesis accepted'\n                          + f'{critical_value:.3} > {test_statistic:.3}\\n'\n                          + f'A_{n} is 0 with certainty {significance}')\n\n\n    def roundRobin(self):\n        \"\"\"\n        Calculate the adjusted R^2 value for all combinations of\n        predictor variables, to determine which predictor variables\n        are best suited to the regression.\n        \"\"\"\n        # Starting with 1 predictor variable up to n variables\n        for n in range(0, self.core_data.ndim):\n            # Generate all combinations of predictor variables\n            for i in itertools.combinations(range(0, self.core_data.ndim), n+1):\n                x = self.core_data.x[:,i]\n                sub_data = self.Data(x, self.core_data.y, self.c)\n\n                print(f'For predictor variables: {i}'\n                      + f': regression = {sub_data.regression}'\n                      + f'R^2 = {sub_data.adjusted_R_sq}')\n\n\n    def plotData(self):\n        \"\"\"\n        Plot each predictor variable against y, showing the correlation\n        co-efficient for each variable. Additionally plots the regression\n        line if there is only one predictor variable.\n\n        \"\"\"\n        for n in range(0,self.core_data.ndim):\n            #plt.subplot(self.core_data.ndim, 1, n+1)\n            plt.figure(n)\n            x = self.core_data.x[:,n]\n            y = self.core_data.y\n            plt.plot(x,y,'o')\n\n            plt.title(f'Data correlation of:'\n                      + f'{self.core_data.correlation[n]:.4}',\n                      fontsize=20)\n            plt.ylabel(f'{self.headers[-1]}',fontsize=18)\n            plt.xlabel(f'{self.headers[n]}',fontsize=18)\n\n\n        # Regression line only makes sense to plot with 1 predictor variable\n        if self.core_data.ndim == 1:\n            a = self.core_data.regression[[n,self.core_data.ndim]]\n            x_matrix= self.c.addOnesToData(x, x.size, 1)\n            y_hat = np.dot(x_matrix, a)\n            plt.plot(x, y_hat, '-',label='Regression line')\n            plt.legend()\n\n        plt.show()\n\n    def estimateData(self):\n        \"\"\"\n        For a given input list of data, provide an estimate y value,\n        along with a\n        confidence interval: (the interval of the mean y value),\n        prediction_interval: (the interval of predicted values).\n        \"\"\"\n        if self.estimates is not None:\n            n = self.estimates.size // self.core_data.ndim\n            x = self.c.addOnesToData(self.estimates, n, self.core_data.ndim)\n\n            y_hat = np.dot(x, self.core_data.regression)\n\n            fval = self.c.findCriticalFValue(\n                1, self.core_data.df, self.confidence)\n\n            for i in range(0,n):\n                x_n = x[i,:-1]\n\n                mahalanobis_distance = self.c.getMahalanobisDistance(x_n,\n                    self.core_data.x_bar,\n                    self.core_data.ndim,\n                    self.core_data.ndata,\n                    self.core_data.s_matrix)\n\n                # fval, sum_errors_sq, df, ndata, mahalanobis_distance):\n                confidence_interval = self.c.getConfidenceInterval(\n                    self.core_data.sum_errors_sq,\n                    self.core_data.df,\n                    self.core_data.ndata,\n                    mahalanobis_distance,\n                    fval)\n\n                prediction_interval = self.c.getPredictionInterval(\n                    self.core_data.sum_errors_sq,\n                    self.core_data.df,\n                    self.core_data.ndata,\n                    mahalanobis_distance,\n                    fval)\n\n                # TODO: add table formatting before loop\n                print(f'x_n {x_n} y_hat {y_hat[i]:.3}'\n                      + f' ci {confidence_interval[0,0]:.3}'\n                      + f' pi {prediction_interval[0,0]:.3}'\n                      + f' with confidence {self.confidence:.3}')\n\n\n    def populateData(self):\n        if not self.data_populated:\n            self.core_data = self.Data(self.data[:,0:-1],self.data[:,-1],self.c)\n            data_populated = True\n\n    def runAnalysisOfVariance(self, significance):\n         self.ANOVA(self.core_data,significance)\n\n    def __init__(self, data, headers, confidence=0.95, estimates=None):\n        self.data_populated = False\n        self.data = data\n        self.confidence = confidence\n        self.c = mvr_calc.MVRCalculator()\n        self.headers = headers\n        self.estimates = estimates\n\n        self.populateData()\n\n        self.runAnalysisOfVariance(self.confidence)\n        self.roundRobin()\n        self.estimateData()\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python",metastring:'title="mvr_calc.py"',title:'"mvr_calc.py"'},'import numpy as np\nfrom scipy.stats import f\n\nclass MVRCalculator:\n    """\n    Class holds the calculations needed to perform the regression\n    on some data. Used to seperate out the data and calculations.\n    """\n    \n    @staticmethod\n    def searchValue(f, target,\n                    tolerance=0.000001, start=0, step_size=1, damping=0.5):\n        """\n        Finds x for a given target y, for a given linear function f(x).\n        Works iteratively through values of x to find the target f(x)\n        value, once the target is \'found\', the step gets reversed\n        and damped until the target is found within the given tolerance.\n        """\n        def stepDirection(increasing, lower):\n            """\n            Finds whether x should increase of decrease,\n            depending if the f(x) function is an increasing or decreasing\n            function and if f(x_0) is lower than f(x_target)\n            """\n            if (increasing and lower) or (not increasing and not lower):\n                return  1\n            else:\n                return -1\n\n        x,error,a0,a1 = start, tolerance+1, f(start), f(start+step_size)\n        increasing, start_lower = a1 > a0, a0 < target\n\n        step_direction = stepDirection(increasing, start_lower)\n        step = step_direction * step_size\n\n        while abs(error) > tolerance :\n            x = x + step   \n            a = f(x)\n\n            error = target - a\n            lower = error > 0\n\n            new_step_direction = stepDirection(increasing, lower)\n\n            # If true, the target x is between f(x) and f(x-step)  \n            if step_direction != new_step_direction:\n                step_size = damping * step_size\n\n            step = new_step_direction * step_size                \n        return x\n\n    @staticmethod\n    def addOnesToData(x,ndata,ndim):\n        """Adds a column of 1s to a given input vector or matrix"""\n        #if len(x.shape) == 1:\n        #    x = np.expand_dims(x, axis=0)\n        x = x.reshape(ndata,ndim)\n        return np.append(x,np.ones((ndata,1)), axis=1)\n\n    @staticmethod\n    def calcSumProduct(vector1,vector2):\n        """Returns the sum of the product of two vectors"""\n        return np.sum(vector1 * vector2)\n\n    @staticmethod\n    def calcCorrelation(ndim, x_y_variance, x_variance_sq, y_variance_sq):\n        """\n        Calculates the correlation between x and y data\n        for each x dimension\n        """\n        coefficients = np.zeros(ndim)\n        for n in range(0,ndim):\n            coefficients[n] = x_y_variance[n] / np.sqrt(\n                x_variance_sq[n] * y_variance_sq)\n            \n        return coefficients\n\n    @staticmethod\n    def calcRegression(s_matrix,x_matrix,y):\n        """Calculates the regression equation (a_0 -> a_n + b)"""\n        return np.dot(s_matrix, np.dot(x_matrix.T, y))\n\n    @staticmethod\n    def findSMatrix(x_matrix):\n        return np.linalg.inv(np.dot(x_matrix.T,x_matrix))\n\n    @staticmethod\n    def findAdjustedRSquared(sum_errors_sq,y_variance_sq,ndata,df):\n        """\n        Finds R^2, adjusted for the fact that normally R^2 will\n        increase for added predictor variables regardless if the variable\n        is a good predictor or not.\n        """\n        return  1 - ((sum_errors_sq / df) / (y_variance_sq / (ndata - 1)))\n\n    @staticmethod\n    def getMahalanobisDistance(x_n, x_bar, ndim, ndata, s_matrix):\n        """Get the mahalanobis distance of a given x_n"""\n        x = (x_n - x_bar).reshape(ndim,1)\n        return np.dot(x.T,np.dot(s_matrix[:-1,:-1],x)) * (ndata - 1)\n\n    @staticmethod\n    def findCriticalFValue(ndim, df, significance):\n        """\n        Find F distribution values, used as critical values in\n        Analysis of variance tests.\n        """\n        return MVRCalculator.searchValue(lambda z: f.cdf(z,ndim,df),\n                                            significance)\n\n    @staticmethod\n    def getConfidenceInterval(\n            sum_errors_sq, df, ndata, mahalanobis_distance, fval):\n        """\n        Interval range for the mean value of a predicted y, to account\n        for the variance in the population data. With the confidence\n        (e.g. 0.95) determined by fval.\n        """\n        return np.sqrt(fval\n                       * (1/ndata + mahalanobis_distance / (ndata -1))\n                       * (sum_errors_sq / df))\n\n    @staticmethod\n    def getPredictionInterval(\n            sum_errors_sq, df, ndata, mahalanobis_distance, fval):\n        """\n        Interval range to give a probable range of future values.\n        This range will be higher than the confidence interval,\n        to account for the fact that the mean predicted value\n        can vary by the confidence value, and then additionally\n        the value can vary from that mean.\n        """\n        return np.sqrt(fval\n                       * (1 + 1/ndata + mahalanobis_distance / (ndata - 1))\n                       * (sum_errors_sq / df))\n\n    @staticmethod\n    def getMatrixWidth(v):\n        """Function to find the width of a given numpy vector or matrix"""\n        if len(np.shape(v)) > 1:\n            return np.shape(v)[1]\n        else:\n            return 1\n\n    @staticmethod\n    def autoCorrelationTest(y_error, sum_errors,sq):\n        """\n        Check for auto correlation in our y data using the\n        Durbin-Watson statistic, a result lower than 1\n        may indicate the presence of autocorrelation.\n        """\n        residual = y_error[1:] - y_error[:-1]\n        return (MVRCalculator.calcSumProduct(residual, residual)\n                / sum_errors_sq)\n\n    @staticmethod\n    def calcAverage(m):\n        return np.mean(m,axis=0)\n\n    @staticmethod\n    def calcVariance(v,v_bar):\n        return v - v_bar\n    \n    @staticmethod\n    def calcTestStatisticAllX(y_variance_sq,sum_errors_sq,ndim,df):\n        """\n        Calculate the test statistic for the analysis of variance\n        where the Null hypothesis is that the population A_1 -> A_n\n        are all equal to 0. Such that the null hypothesis gets\n        rejected if any A_x != 0.\n        """\n        return (((y_variance_sq - sum_errors_sq) / ndim)\n                / (sum_errors_sq / df))\n\n    @staticmethod\n    def calcTestStatisticSingleX(regression, s_matrix, sum_errors_sq, n, df):\n        """\n        Calculate the test statistic for the analysis of variance\n        where the Null hypothesis is that the population A_n is 0.\n        Such that the null hypothesis gets rejected if A_n != 0.\n        """\n        return (regression[n]**2 / s_matrix[n,n]) / (sum_errors_sq / df)\n')))}p.isMDXComponent=!0}}]);